# SynODC: Utilizing the Syntactic Structure for Outlier Detection in Categorical Attributes

The problem of outlier detection is a long-standing problem, where outliers affect the data quality significantly. Because of that, machine learning models tend to produce inaccurate decisions and poor predictions. While detecting outliers in numerical data has been extensively studied, few attempts were made to solve the problem of detecting outliers in attributes with categorical values. In this paper, we introduce SynODC for detecting categorical outliers in relational (tabular) datasets by utilizing the syntactic structure of the values. For a given attribute, SynODC identifies a set of patterns that represent the majority of the values as dominating patterns. Data values that do not match (i.e. cannot be generated by) one of the dominating patterns are declared as outliers. Our target is to construct, for each attribute, a minimal set of dominating patterns that are expressive enough to represent the different formats of the values in the attribute. 
In order to do that, we define a new distance metric that generalizes the Levenshtein distance to measure the distance between the patterns and combine similar patterns to maintain compact representations of the attributes. 

## Running the tool
The tool is implemented in Python and uses dash library for the graphical interface. To run the tool, proceed with the following instructions:
* get the code first:
```
# Download the code from https://anonymous.4open.science/r/SynODC-D1BE

cd SynODC
```
* Install the required libraries
```
pip install -r requirements.txt
```
#### Running from the command line

```
python SynODC table_name min_coverage max_len min_cov_per_pattern [default|custom]
```

#### Running the GUI

```
python GUI.py 
```

## Parameters

SynODC has four main parameters: 

* minimum coverage of the dominant patterns ($\xi$): we add more patterns to the set of dominant patterns until the ratio of the generated values using the dominant patterns to the total number of values is greater than $\xi$; the rest of the patterns are flagged as potential erroneous patterns.
* The max allowed length for values ($L$): no exhaustive search for patterns in values with length greater than $L$; instead, we use only two patterns (the pattern of the original characters and the fully generalized pattern).
* The minimum coverage per dominant pattern ($\gamma$): for a pattern $p_i$ to be flagged as dominant pattern, the ratio of the generated values by $p_i$ to the total number of values in the attribute must be greater than $\gamma$; Parameters $\xi$ and $\gamma$ are considered based on the fact that outliers are rare events and should be minority in any given dataset.
* The similarity measure: we use the customized Levenshtein distance to measure the distance between patterns. However, the original Levenshtein distance can also be tested in filtering the false positives/negatives. In the command line, this can be set using `default` for original Levenshtein distance and `custom` for the customized Levenshtein distance. 

  
## Testing the tool

Download the [hospital dataset](https://github.com/BigDaMa/raha/tree/master/datasets/hospital) and use the following parameters:

```
Min coverage = 99
Max Length = 15
Min Cov. per Pattern = 0.1
Distance Function = Cust. Lev. distance
```

